{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing required libraries\n",
    "\n",
    "We will need pandas , numpy and seaborn to extract, process and plot the data sequentially."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import normalize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Extraction , Processing and Feture Extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our Dataset  is divided into multiple sections as :\n",
    "\n",
    "- Baseline Features: Column-3 to Column-23\n",
    "- Intensity Parameters: Col24 to Col26\n",
    "- Formant Frequencies: Col27 to Col30\n",
    "- Bandwidth Parameters: Col31 to Col34\n",
    "- Vocal Fold: Col35 to Col56\n",
    "- MFCC: Col57 to Col140\n",
    "- Wavelet Features: Col141 to Col322\n",
    "- TQWT Features: Col323 to Col754\n",
    "- Class: Col755\n",
    "\n",
    "*Refer Research Page: 6 - 9*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our analysis, we will be using the following features:\n",
    "1. Baseline Features\n",
    "2. Intensity Parameters\n",
    "3. Formant Frequencies\n",
    "4. Bandwidth Parameters\n",
    "5. MFCC Features\n",
    "6. Class\n",
    "\n",
    "Which in total we have 45 features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Helper methods for data-extraction\n",
    "from helpers import read_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'orginal_dataset/pd_speech_features.csv'\n",
    "dataframe = read_data(filename)\n",
    "\n",
    "y = dataframe['class']\n",
    "original_df = dataframe.drop(['class'], axis=1)\n",
    "\n",
    "#basic information of dataset\n",
    "original_df.info()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, we have dataframe with 45 features / columns and 756 datapoints / rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing and Feature Extraction\n",
    "\n",
    "[Ref 1 : Working with Numerical Data](https://machinelearningmastery.com/feature-selection-with-numerical-input-data/)\n",
    "[Ref 2 : Feature Selection Examples](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py)\n",
    "[Ref 3 : Correlation and Standarization](https://stats.stackexchange.com/questions/220724/can-i-test-for-correlation-between-variables-before-standardize-them)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variance check of every columns\n",
    "variance_df = original_df.var().round(5)\n",
    "variance_df = variance_df.sort_values(ascending=True)\n",
    "variance_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see first 6 columns with very low variance and will be excluded from our analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Removing columns with low variance\n",
    "var_filter_df = original_df.drop(original_df.columns[0:6], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier Detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are using Density based clustering model (DBSCAN) to find the outliers on the dataset. Using threshold of 90% quantile to get the farthest point from the cluster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "model_obj = hdbscan.HDBSCAN(alpha=0.01, min_samples=5,\n",
    "                            min_cluster_size=10,\n",
    "                            cluster_selection_epsilon=0.01)\n",
    "model_obj.fit(normalize(var_filter_df,mode='minmax'))\n",
    "\n",
    "threshold = pd.Series(model_obj.outlier_scores_).quantile(0.9)\n",
    "outliers = np.where(model_obj.outlier_scores_ > threshold)[0]\n",
    "outliers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "var_filter_df = var_filter_df.drop(outliers, axis=0).reset_index(drop=True)\n",
    "y = y.drop(outliers, axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Type 1 - Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Type 1** Data-preprcessing and Feature Selection uses methods like , self correlation and  F-test based selection."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers import correlation_heatmap, get_feature_correlation, to_remove_columns\n",
    "# Drawing a heatmap of correlation between features\n",
    "correlation_heatmap(var_filter_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see a few features pair with high correlation (positive or negative) with each other. We will remove these feature analysing their correlation with target variable.\n",
    "\n",
    "We are filtering the features with correlation greater than *0.8*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "feature_correlation_df = get_feature_correlation(var_filter_df, threshold)\n",
    "feature_correlation_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see there are 10 features pari with correlation greater than *0.8*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "remove_corr_columns = to_remove_columns(var_filter_df, y, feature_correlation_df)\n",
    "print(f'We are require to remove {len(remove_corr_columns)} columns, which are: {remove_corr_columns}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_filter_df = var_filter_df.drop(remove_corr_columns, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lets visualize correlation matrix\n",
    "correlation_heatmap(corr_filter_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-collinearity Check\n",
    "\n",
    "We will be using VIF from statsmodels library to check the multi-collinearity.\n",
    "*VIF_threshold = 10*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_threshold = 30\n",
    "\n",
    " # VIF dataframe\n",
    "vif_data = pd.DataFrame(corr_filter_df.columns, columns=['Features'])\n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(corr_filter_df, i)\n",
    "    for i in range(len(corr_filter_df.columns))\n",
    "]\n",
    "# Sort VIF in descending order\n",
    "vif_data.sort_values(by='VIF',ascending=False ,inplace=True)\n",
    "vif_data\n",
    "# vif_data[vif_data.VIF > vif_threshold]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see a few features with VIF greater than *10*. So we will remove these features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_remove_vif = vif_data[vif_data.VIF > vif_threshold].Features.tolist()\n",
    "print(f'We are require to remove {len(to_remove_vif)} columns, which are: {to_remove_vif}')\n",
    "\n",
    "multicorr_filter_data = corr_filter_df.drop(to_remove_vif, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multicorr_filter_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class Correlation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correlation with class\n",
    "\n",
    "correlate = np.array([multicorr_filter_data[columns].corr(y) for columns in multicorr_filter_data.columns])\n",
    "corr_df = pd.DataFrame(abs(correlate.round(5)),index=multicorr_filter_data.columns,columns=['Correlation with Class'])\n",
    "corr_df.sort_values(by='Correlation with Class',ascending=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not a single feature has very low correlation with class. So we don't need to remove any feature."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Till now we are left with 16 features excluding *class*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We aren't able to work with following selection tests as,\n",
    "\n",
    "1. Chi2 because it is not applicable for numerical data.\n",
    "2. Mutual Information because of smaller number of features/samples.\n",
    "3. Lasso because of small number of features/samples."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using MinMax Normalization to normalize the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_df = normalize(corr_filter_df,mode='minmax')\n",
    "normalized_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stepwise Selection\n",
    "\n",
    "We will work with backward elimination to select the features. That means we will remove the feature with highest p-value as we go."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_df.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers import BackwardElimination\n",
    "\n",
    "backward_elimination = BackwardElimination(\n",
    "    normalized_df, y, scoring='roc_auc'\n",
    ")\n",
    "backward_elimination.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df_step1 = backward_elimination.get_transformed_data()\n",
    "final_df_step1.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Type 2 - Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Type 2** Data-preprcessing and Feature Selection uses methods like , PCA-based selection, PCA-based selection with correlation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers import pca_dataframe\n",
    "\n",
    "pca_df = pca_dataframe(\n",
    "    normalize(var_filter_df,'minmax'),\n",
    "    prob=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With PCA Dimensionality Reduction , we get *31* independent features out of *38* features.\n",
    "These 31 features can represent 90% of the original features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class Correlation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correlation with class\n",
    "\n",
    "class_correlate = np.array([pca_df[columns].corr(y) for columns in pca_df.columns])\n",
    "corr_df_2 = pd.DataFrame(\n",
    "    abs(class_correlate.round(5)),index=pca_df.columns, columns=['Correlation with Class']\n",
    ")\n",
    "corr_df_2.sort_values(by='Correlation with Class',ascending=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not a single feature has very low correlation with class. So we don't need to remove any feature."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stepwise Selection\n",
    "\n",
    "We will work with backward elimination to select the features. That means we will remove the feature with highest p-value as we go."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers import BackwardElimination\n",
    "\n",
    "backward_elimination = BackwardElimination(\n",
    "    pca_df, y, scoring='roc_auc'\n",
    ")\n",
    "backward_elimination.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df_step2 = backward_elimination.get_transformed_data()\n",
    "final_df_step2.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finalizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 27 features in Type-1 pre-processed dataset and 30 features inType-2 pre-processed dataset. That is difference of 17 and 14 features from original dataset respectively.\n",
      "\n",
      "We have removed 17 features: {'apq11Shimmer', 'ppq5Jitter', 'apq5Shimmer', 'apq3Shimmer', 'ddaShimmer', 'RPDE', 'meanPeriodPulses', 'meanIntensity', 'minIntensity', 'locShimmer', 'numPulses', 'rapJitter', 'meanAutoCorrHarmonicity', 'meanHarmToNoiseHarmonicity', 'mean_MFCC_11th_coef', 'ddpJitter', 'locAbsJitter'} from Type-1 pre-processed dataset \n",
      " and 14 features from Type-2 pre-processed dataset.\n"
     ]
    }
   ],
   "source": [
    "or_len = len(original_df.columns)\n",
    "s1_len = len(final_df_step1.columns)\n",
    "s2_len = len(final_df_step2.columns)\n",
    "\n",
    "col_diff_S1 = set(original_df.columns) - set(final_df_step1.columns)\n",
    "\n",
    "\n",
    "print(f'We have {s1_len} features in Type-1 pre-processed dataset and {s2_len} features in'\n",
    "      f'Type-2 pre-processed dataset. That is difference of {or_len - s1_len} and '\n",
    "      f'{or_len - s2_len} features from original dataset respectively.')\n",
    "print()\n",
    "print(f'We have removed {len(col_diff_S1)} features: {col_diff_S1} from Type-1 pre-processed dataset \\n and {or_len - s2_len} features from Type-2 pre-processed dataset.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "final_df_step1['class'] = y\n",
    "final_df_step2['class'] = y\n",
    "\n",
    "final_df_step1.to_csv('processed_dataset/final_data_S1.csv', index=False)\n",
    "final_df_step2.to_csv('processed_dataset/final_data_S2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}